# Statistical Machine Learning (SML) course

Statistical Machine Learning refers to a set of tools for inferring, predicting and extracting common features and hidden structures in massive and complex datasets available in diverse scientific disciplines, such as marketing, e-commerce, economics, finance, social media, genomics, neuroscience, bioinformatics, meteorology, transport and telecommunications, just to name a few, where the explosion of “Big Data” problems has become the rule rather than exception. The field encompasses many theories, methods and algorithms, for instance, dimensionality reduction, clustering, and outlier detection techniques in high-dimensional data settings, estimation and testing under sparsity, tree-based methods, bagging, boosting, random forests, neuralnetworks, support vector machines, and graphical methods, all these developed for classification and regression problems. Thus, numerous public and private organizations are needing professionals with statistical learning and computer programming skills to analyze their datasets. The goal of this intermediate course is to provide a self-contained overview of key concepts underlying many of statistical learning and data mining tools. For each method, a description of intuitive ideas, necessary mathematical and statistical details, and algorithmic implementation is provided.

Required prerequisites for this course include multivariate calculus, linear algebra, optimization theory, basic probability, inferential statistics (in particular, regression modelling, maximum likelihood, and stochastic convergence). Other important themes in Machine Learning such as Gaussian processes, Bayesian machine learning, deep learning, reinforcement learning, active learning, semi-supervised learning, graphical modeling, and causal inference will be covered in other courses.

## Objective:

The main objective of the course is to provide a complete overview of the basic theory concerning to the main tools (models and algorithms) related to statistical learning and data mining.

## Software:

Two well-known programming languages for machine learning will be used for the applications during the course, Python (https://www.python.org/) and R (http://www.r-project.org/). Both languages are the most used among the scientific data community for data mining, statistical analysis, predictive analysis, processing and visualization information, etc.

## Contents

1. Statistical learning
   
   1.1. What is statistical learning?
   1.2. Supervised versus unsupervised learning
   1.3. Classification and regression problems
   1.4. The trade-off between prediction and inference
   1.5. Performance evaluation
   1.6. The bias-variance trade-off
   1.7. Resampling methods: cross-validation and bootstrap
   
 2. Generalized linear models with penalties
    
    2.1. Introduction to Generalized Linear Models (GLM’s)
    2.2. Shrinkage methods
      2.2.1. Ridge regression
      2.2.2. Lasso
      2.2.3. Elastic net
    2.3. Least angle regression and coordinate descent algorithms

  3. Introduction to nonparametric regression

    3.1. k-nearest neighbors regression
    3.2. Kernel smoothing
    3.3. Local polynomial regression
    3.4. Regression and smoothing splines
    3.5. Orthogonal series (projection estimators)

  4. Decision trees

    4.1. Classification trees
    4.2. Regression trees
    4.3. Induction of decision trees
    4.4. Pruning

  5. Ensemble methods
    
    5.1. Bagging
    5.2. Random forests
        5.2.1. Randomizing tree construction
        5.2.2. Generalization error
        5.2.3. Strength and correlation
        5.2.4. Variable importance measures
    5.3. Boosting
        5.3.1. Discrete adaptive boosting
        5.3.2. The forward stagewise algorithm
        5.3.3. Forward stagewise additive logistic model
        5.3.4. Gradient boosting
            5.3.4.1. Gradient tree boosting
        5.3.5. Regularization
        5.3.6. Stochastic gradient boosting
            5.3.6.1. Stochastic gradient tree boosting
        5.3.7. Extreme gradient boosting

  6. Support vector machines

    6.1. Separating hyperplanes
    6.2. Kernels and reproducing kernel Hilbert spaces
    6.3. Support vectors for classification (two-class and multi-class)
        6.3.1. $`\sqrt{2}`$-support vector classification
        6.3.2. $`\sqrt{2}`$-support vector classification
    6.4. Support vectors for regression
        6.4.1. -support vector regression
        6.4.2. -support vector regression

   7. Artificial Neural Networks

    7.1. Perceptrons
      7.1.1. Single-layer perceptrons
      7.1.2. Multi-layer perceptrons
    7.2. The gradient descent algorithm
    7.3. The backpropagation algorithm
    7.4. Activation and loss functions
    7.5. Fitting neural networks
      7.5.1. Starting values
      7.5.2. Overfitting
      7.5.3. Input scaling
        7.5.4. Number of hidden units and layers
    7.6. Relating statistical methods

8. Linear dimensionality reduction
  
    8.1. Principal component analysis
    8.2. Factor analysis
    8.3. Multidimensional scaling
    8.4. Sparse PCA

   9. Clustering
    
    9.1. Proximity and dissimilarity matrices
    9.2. $K$-means clustering
    9.3. $K$-medoids clustering
    9.4. Spectral clustering
    9.5. Hierarchical clustering
    9.6. Model-based clustering
    9.7. Support-vector clustering
    9.8. Density-based clustering
